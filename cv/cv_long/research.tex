\section{Research}

\cvitem{}{I have been a member of the CMS collaboration since 2000. My
  research interests can be broadly summarised as follows: the
  delivery of large-scale detector readout systems; the extraction of
  physics observables from data recorded by the CMS detector; and
  searches for evidence of new fundamental physics at the LHC, which
  is my current research focus. The following summarises my research
  activities, presented in chronological order.
%  I have an excellent profile in both the detector and physics
%  communities within the CMS Collaboration and beyond. 
}

\vspace{0.1cm}
\subsection{Research interests as an undergraduate student}

\cvitem{1998-1999}{\textbf{Neutrino oscillations}: I performed an
  analysis of data collected by the \texttt{NOMAD} experiment based at
  CERN to search for evidence of $\nu_\mu \rightarrow \nu_\tau$
  neutrino oscillations, while based at the University of Padova,
  Italy, as an \texttt{ERASMUS} student.}

\cvitem{1999}{\textbf{Cosmic ray astronomy}: I prototyped a
  \texttt{GEANT}-based simulation to model the detection of cosmic ray
  showers in the ground-based large-area detector array of the
  \texttt{AUGER} experiment, as a summer student at the University of
  Leeds.}

\cvitem{2000}{\textbf{Silicon sensors}: I performed quality assurance
  tests on large-area pixellated silicon detectors for X-ray
  spectroscopy, as part of the \texttt{IMPACT} project, as a summer
  student at Imperial College London.} 

\vspace{0.1cm}
\subsection{Experience with electronic systems and detector performance}

\cvitem{2000--2010}{\textbf{The CMS Silicon Strip Tracker}: The
  Imperial HEP group was responsible for key design features of the
  CMS Tracker detector, an 80M CHF (\textsterling 60M) project,
  including an all-silicon design and a readout system based around a
  custom front-end ASIC providing analogue data via optical links to
  an off-detector digital processing board with FPGA technology. The
  silicon strip tracker (SST) is one of the most complex detectors of
  its type, comprising 10M readout channels. The group was responsible
  for the design and characterisation of several key components, as
  well as commissioning the final system and providing long-term
  operational support. I have led several key activities within the
  CMS Tracker Collaboration, comprising $\sim$500 members. My
  experience is detailed below.  }

\cvitem{2000--2005}{\textbf{Characterisation of front-end ASICs for
    detector readout}: I authored procedures for a production test
  bench installed at Imperial College that provided automated,
  large-scale screening and characterisation of 76000 front-end
  readout ASICs prior to their integration within the CMS Tracker
  detector~\cite{pub-apvs, conf-twepp2005, conf-twepp2004-5,
    conf-twepp2002-2, prelim-10}.}

\cvitem{2002--2005}{\textbf{Radiation effects on front-end
    electronics}: I led a three-year study on the effect of
  highly-ionising nuclear interactions within silicon sensors on the
  performance of the Tracker readout system. I was the first to
  quantify the effect through an analysis of beam test
  data~\cite{conf-twepp2002-1, prelim-12}. I was subsequently tasked
  with defining and executing a comprehensive measurement programme
  during a dedicated beam test in 2002 that involved a team of forty
  researchers across multiple institutes~\cite{prelim-11}. I was the
  corresponding author for the subsequent publication~\cite{pub-hips},
  which was the first publication to be signed by the newly formed CMS
  Tracker Collaboration (of approximately 500 members). More recently,
  firmware implementations of algorithms designed to minimise the
  effects of highly-ionising nuclear interactions, developed during my
  PhD studies~\cite{thesis}, are being investigated for use within the
  off-detector electronics of the CMS Tracker during LHC Heavy Ion
  collisions. I have also contributed (in a minor role) to performance
  studies of total dose effects~\cite{conf-sirad2004}.  }

\cvitem{2004--2005}{\textbf{Performance testing of back-end readout
    electronics}: I evaluated the performance of off-detector digital
  boards that utilise FPGAs, the Front-End Drivers (FEDs) of the CMS
  Tracker, both with test stands in the laboratory and prototype
  readout systems during dedicated beam tests~\cite{pub-feds,
    conf-twepp2004-1, conf-twepp2004-2, conf-twepp2004-3}. }

\cvitem{2005--2009}{\textbf{Project management of data acquisition
    software}: I was responsible for the integration of the CMS
  offline reconstruction and online data acquisition software
  frameworks~\cite{conf-chep2007-2, conf-chep2007-4}, which was, at
  the time, novel work within the detector projects of CMS. This
  approach was required to provide a scalable solution for
  commissioning and calibrating the ten million channels of the CMS
  Tracker detector, one of the most complex detector systems in the
  field.  }

\cvitem{2005--2009}{\textbf{Distributed analysis framework and
    detector calibration}: I led the design and implementation of a
  large-scale distributed analysis framework that optimised
  $\mathcal{O}(10^5)$ configuration parameters and determined
  $\mathcal{O}(10^7)$ calibration constants for the control and
  readout systems~\cite{prelim-9, conf-vertex2007,
    conf-twepp2006-1}. I also developed the software infrastructure to
  handle conditions data relevant for the Tracker, the interfaces to
  the online configuration database within CMSSW, and the
  online-to-offline transfer protocols that were used to transfer
  calibration constants for use by the online (High-Level Trigger) and
  offline event reconstruction with the CMSSW
  framework~\cite{conf-chep2007-4}.  }

\cvitem{2006--2010}{\textbf{Large-scale integration tests and detector
    commissioning}: The operational and physics performance of the
  Tracker is outstanding, providing near-perfect channel efficiencies
  and high-quality, calibrated physics data. I was a key member of a
  detector integration and commissioning team that help to deliver
  this high-performance instrument. The aforementioned frameworks were
  used during several early ``vertical slice'' system tests, including
  the Magnet Test Cosmic Challenge~\cite{pub-magnet-test}, and a 15\%
  Sector Test at the CMS Tracker Integration Facility comprising both
  hardware performance studies~\cite{pub-tracker-perf} and cosmic ray
  studies~\cite{pub-tracker-cosmics}. The frameworks were also used to
  commission the newly-installed complete detector in the LHC
  experimental cavern and throughout Run~1 of the
  LHC~\cite{pub-cosmics}. The framework and procedures are still
  during LHC Run~2, and I am currently involved in optimisations of
  the softare framework to handle a multi-threaded environment.}

\cvitem{2007--2008}{\textbf{VME-based monitoring of non-sparsified
    data}: I contributed to the design and implementation of the data
  acquisition and reconstruction software required to handle a
  VME-based "spy" data stream that provided non-sparsified data during
  detector operations for calibration and monitoring purposes that was
  used throughout Run~1~\cite{conf-chep2007-3}.}

\vspace{0.1cm}
\subsection{Experience in CMS physics performance}

\cvitem{2006--2007}{\textbf{Raw data formats and low-level
    reconstruction}: I designed and implemented the software that
  handles the raw sparsified data from the CMS Tracker and
  reconstructs signals from the passage of charged
  particles~\cite{conf-chep2007-1}, which are used as inputs to the
  track reconstruction algorithms. The software is able to parse the
  sparsified data format produced by the FEDs for input to the event
  reconstruction algorithms, as well as process the dedicated
  non-sparsified data packets that are used for calibration
  procedures. }

\cvitem{2007--2009}{\textbf{Regionalised low-level reconstruction for
    triggering}: I engineered a highly-optimised, regionalised
  approach to the low-level data processing steps that seed the track
  reconstruction algorithms within the CMS High-Level Trigger
  system~\cite{pub-hlt}. The approach was a key reason why the working
  group achieved one of its performance goals during early stress
  tests of the system. The method is still in use today.}

\cvitem{2008-2009}{\textbf{Jet-plus-tracks reconstruction algorithm}:
  I also contributed to the development of a jet algorithm that
  provides improved performance in terms of energy scale, and energy
  and angular resolutions, by using track-based information. The
  algorithm was extensively used by the CMS collaboration throughout
  Run~1~\cite{pub-jec, prelim-7}.}

\cvitem{2012}{\textbf{High-level trigger algorithms with energy sums}:
  I contributed to the design, implementation, and monitoring of a
  topology-based jet trigger aimed at searches for
  supersymmetry~\cite{pub-alphat3}. }

\cvitem{}{\textbf{Member of physics commissioning task force}: I was a
  member of a team that was tasked with characterising the physics
  performance of the new and hugely complex CMS detector that was
  operating at a previously unexplored collider energy and luminosity
  frontier. From the start of Run~1 of the LHC, the team commissioned
  the use of physics objects~\cite{pub-jec, prelim-3, prelim-4,
    prelim-5} and analysis methods~\cite{prelim-2} that served as
  crucial ingredients to numerous analysis teams within the
  Collaboration.}

\vspace{0.1cm}
\subsection{Experience in searches for new physics phenomena}

\cvitem{2009--2013}{\textbf{Search for supersymmetry at a new energy
    frontier}: I was a leading author of an analysis that resulted in
  the first published search for supersymmetry from the
  LHC~\cite{pub-alphat1}. The new energy frontier probed by the LHC at
  the start of Run~1 provided the most significant increase in
  discovery potential for heavy supersymmetric particles in recent
  years. The result was high impact and a huge achievement for the
  team considering the difficulties of delivering a search based on
  data collected by a new detector operating at a new collider
  facility. Large regions of model parameter space, inaccessible by
  the previous-generation particle accelerators, were demonstrated to
  be incompatible with the data collected by the experiment. }

\cvitem{2009--2013}{\textbf{Further high-impact publications from LHC
    Run~1}: The work of the analysis team subsequently yielded a
  further three high-impact publications based on Run~1
  data~\cite{pub-alphat2, pub-alphat3, pub-alphat4}. A fifth letter
  was recently submitted for publication~\cite{pub-alphat5}. I was a
  leading author and editor of all publications. These papers are
  supported by additional simulation-based studies of the analysis
  strategy~\cite{prelim-6} and the re-interpretation of published
  results with new physics models~\cite{pub-sms, prelim-1}. The
  aforementioned papers have significant consequences for the field of
  high energy physics and are of importance to the wider physics
  community. Each paper contained world-leading results at the time of
  publication. Their impact can be gauged by the citations
  accumulated: two of the four papers are considered "famous"
  (i.e. $>$250 citations) and the four papers have collectively
  accummulated in excess of 800 citations, which, for an individual
  research team, ranks very highly at the LHC. The impact of the
  second paper~\cite{pub-alphat2} was acknowledged by being
  highlighted in an APS Physics Viewpoint
  article~\cite{viewpoint}. The Run~1 results also feature in the
  Particle Data Group Review of Particle Physics~\cite{pdg}. }

\cvitem{2014-2015}{\textbf{Preparations for LHC Run~2}: It is expected
  that the data collected during Run~2 will provide answers to some of
  the most important questions faced by the high energy physics
  community, such as the nature of Dark Matter and whether
  supersymmetry can provide a viable solution to the hierarchy problem
  and explain the light mass of the Higgs boson. My team that I
  coordinate will play a crucial role in answering these questions. }

\cvitem{}{During 2014, I coordinated the significant preparatory work
  required prior to the start of Run~2. The team was identified as a
  priority CMS Early Analysis and was subsequently tasked with
  demonstrating readiness and projected sensitivity for Run~2 through
  a six-month review process based on simulation-based studies of the
  analysis strategy. Other preparatory activities included: the
  investigations into new algorithms to be used in the CMS Level-1
  trigger system that are of interest to a range of physics analysis
  groups; the definition of the trigger strategy required to collected
  the necessary data samples for our specific analysis; and the
  training of new postdoctoral and postgraduate students. }

\cvitem{}{I also oversaw a broadening of the search strategy in order
  to provide sensitivity to broad classes of models that conjecture
  the production of Dark Matter at the LHC without the need for a
  supersymmetric framework. The search provides complementary
  sensitivity with respect to non-collider experiments performing
  direct and indirect searches for dark matter. The changes also
  provide improved sensitivity to the invisible decay width of the
  Higgs boson, particularly when assuming production via gluon
  fusion.}

\cvitem{2015--present}{\textbf{Searches for new phenomena during LHC
    Run 2}: The team was successful in delivering again an early
  result based on the data collected in 2015~\cite{prelim-16}, which
  disfavours additional regions of the supersymmetric model parameter
  space, particularly those characterised by a TeV-scale gluino and a
  sub-TeV neutralino (a Dark Matter candidate). The team is currently
  preparing for publication, and promoting interpretations of their
  results within multiple analysis groups in CMS. The team has also
  released a preliminary result based on data recorded by CMS in
  2016~\cite{prelim-18}. This year, the LHC is, for the first time,
  operating at design specification, which is extremely important for
  the discovery potential of new physics. In 2016, the machine is
  expected to deliver more than a factor 15 gain in integrated
  luminosity with respect to 2015. The preliminary result is an
  important milestone towards a publication of an analysis of this
  dataset. }
